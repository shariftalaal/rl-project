{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef65dca",
   "metadata": {},
   "source": [
    "# Super Mario Bros RL Notebook\n",
    "## Rohan H, Talaal S, Thang N, Yuet W\n",
    "### University of Bath\n",
    "\n",
    "![SegmentLocal](mario!.gif \"segment\")\n",
    "(we are going to have to delete him sadly...)\n",
    "\n",
    "\n",
    "### How to run:\n",
    "The official website for the game environment can be found here: https://pypi.org/project/gym-super-mario-bros/\n",
    "\n",
    "In a nutshell, you will need:\n",
    "- Python 3.5/3.6/3.7/3.8 (I have tested on 3.7)\n",
    "- gymnasium (gym is deprecated)\n",
    "- ipykernel for running the notebook\n",
    "- gym-super-mario-bros \n",
    "- other essential packages/libraries like NumPy\n",
    "- an average computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4068cc",
   "metadata": {},
   "source": [
    "### Check if the environment works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcfa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env_example = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='human')\n",
    "env_example = JoypadSpace(env_example, SIMPLE_MOVEMENT)\n",
    "\n",
    "test = False\n",
    "done = True\n",
    "if test:\n",
    "    for step in range(1000):\n",
    "        if done:\n",
    "            obs, info = env_example.reset()\n",
    "        obs, reward, terminated, truncated, info = env_example.step(env_example.action_space.sample())\n",
    "        done = not terminated or truncated\n",
    "\n",
    "env_example.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d1c82",
   "metadata": {},
   "source": [
    "# /// DQN Algorithm ///\n",
    "\n",
    "## Importing Libraries, Defining Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df5bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "# Rohan's Cell\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# example here: https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers.frame_stack import FrameStack\n",
    "from gym.core import ObservationWrapper\n",
    "import gym_super_mario_bros.actions as actions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "#import torch.nn.functional as func\n",
    "#from torchvision import transforms as T\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "#from itertools import count\n",
    "#import itertools\n",
    "\n",
    "import cv2\n",
    "\n",
    "def device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "system = device()\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "    ['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN_full(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQN_full, self).__init__()\n",
    "        self.convolution = nn.Sequential( # Convolutional layers\n",
    "        nn.Conv2d(shape[0], 16, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.fully_connected = nn.Sequential( # 64 neurons -> 512 neurons -> \"n\" actions \n",
    "            nn.Linear(output_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, actions_n)\n",
    "        )\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution(x).view(x.size(0), -1)\n",
    "        return self.fully_connected(out)\n",
    "    \n",
    "\n",
    "class ReShape(ObservationWrapper): # Put RGB channel as first argument\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            low=self.observation_space.low.min(),\n",
    "            high=self.observation_space.high.max(),\n",
    "            shape=(obs_shape[2], obs_shape[0], obs_shape[1]),\n",
    "            dtype=env.observation_space.dtype,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.transpose(observation, (2, 0, 1))\n",
    "\n",
    "class SmallBox84(ObservationWrapper): # Resize to 84x84\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=1.0,\n",
    "            shape=(1, 84, 84),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "    def observation(self, observation):\n",
    "        return cv2.resize(observation, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "class GreyScale(ObservationWrapper): # Convert to greyscale\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=1.0,\n",
    "            shape=(1, env.observation_space.shape[1], env.observation_space.shape[2]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "    def transform(self, observation):\n",
    "        return np.dot(observation[..., :3], [0.299, 0.587, 0.114]).reshape(1, observation.shape[0], observation.shape[1])\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.transform(observation)\n",
    "        return observation.astype(np.float32) / 255.0\n",
    "    \n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='robot')\n",
    "env = JoypadSpace(env, actions.RIGHT_ONLY)\n",
    "env = ReShape(env)\n",
    "env = GreyScale(env)\n",
    "env = SmallBox84(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b99c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95 # discount reward\n",
    "update = 0.004 # learning rate\n",
    "epsilon_start = 0.9 # initial exploration rate\n",
    "epsilon_end = 0.005 # final exploration rate\n",
    "e_decay = 2500 # decay rate\n",
    "action_n = env.action_space.n\n",
    "obs, info = env.reset()\n",
    "\n",
    "network = DQN_full(obs.shape, action_n).to(system)\n",
    "\n",
    "optimiser = opt.AdamW(network.parameters(), lr=update)\n",
    "memory = ReplayMemory(1000) \n",
    "\n",
    "steps_passed = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_passed\n",
    "    sample = random.random()\n",
    "    steps_passed += 1\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * steps_passed / e_decay)\n",
    "    if sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return network(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=system, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba0889",
   "metadata": {},
   "source": [
    "## Model Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "876ad882",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50 #UPDATE\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size: # take 250 of 10000 to update model params\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=system, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=system)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = network(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(network.parameters(), 15)\n",
    "    optimiser.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f0dd3",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e23763a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  0\n",
      "tensor([164.])\n",
      "Starting Episode  1\n",
      "tensor([175.])\n",
      "Starting Episode  2\n",
      "tensor([155.])\n",
      "Starting Episode  3\n",
      "tensor([148.])\n",
      "Starting Episode  4\n",
      "tensor([136.])\n",
      "Starting Episode  5\n",
      "tensor([141.])\n",
      "Starting Episode  6\n",
      "tensor([170.])\n",
      "Starting Episode  7\n",
      "tensor([171.])\n",
      "Starting Episode  8\n",
      "tensor([137.])\n",
      "Starting Episode  9\n",
      "tensor([215.])\n",
      "Starting Episode  10\n",
      "tensor([196.])\n",
      "Starting Episode  11\n",
      "tensor([197.])\n",
      "Starting Episode  12\n",
      "tensor([172.])\n",
      "Starting Episode  13\n",
      "tensor([168.])\n",
      "Starting Episode  14\n",
      "tensor([184.])\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "if system == torch.device(\"cuda\"):\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 15\n",
    "\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "steps_passed = 0\n",
    "\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state.copy(), dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    for i in range(1, 100):  # count():\n",
    "        action = select_action(state)\n",
    "        obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=system)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            episode_steps.append(i+1)\n",
    "            episode_rewards.append(total_reward)\n",
    "            print(total_reward)\n",
    "            break\n",
    "    \n",
    "    episode_steps.append(i+1)\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(total_reward)\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "torch.save(network.state_dict(), 'mario_DQN.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f2571",
   "metadata": {},
   "source": [
    "# /// A2C Algorithm ///\n",
    "\n",
    "To save time, we will use some of the same classes and functions from DQN. Make sure you run all of the prior code (except the training loop for DQN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31612832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  0\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'CriticNetwork' object has no attribute 'fully_connected'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12224\\1926413246.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    105\u001b[0m     \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    106\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m100\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# count():\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 107\u001b[1;33m         \u001b[0maction_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0ma2c_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    108\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultinomial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_probability\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mprob_log\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction_probability\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_12224\\1926413246.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     45\u001b[0m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 46\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     47\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     48\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1268\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mmodules\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1269\u001b[0m         raise AttributeError(\"'{}' object has no attribute '{}'\".format(\n\u001b[1;32m-> 1270\u001b[1;33m             type(self).__name__, name))\n\u001b[0m\u001b[0;32m   1271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1272\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'Module'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'CriticNetwork' object has no attribute 'fully_connected'"
     ]
    }
   ],
   "source": [
    "steps_passed = 0\n",
    "eps_rewards = []\n",
    "eps_steps = []\n",
    "advantage = 0.0\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.convolution = nn.Sequential( # combined convolutional layers\n",
    "        nn.Conv2d(shape[0],32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.actor = nn.Sequential(  \n",
    "            nn.Linear(output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def get_values(self, x):\n",
    "        x = self.convolution(x)\n",
    "        actor = self.actor(x).view(x.size(0), -1)\n",
    "        critic = self.critic(x).view(x.size(0), -1)\n",
    "        return actor, critic\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.convolution(x).view(x.size(0), -1)\n",
    "        return self.actor(out)\n",
    "\n",
    "\n",
    "batch_size = 250\n",
    "def optimize_a2c():\n",
    "    if len(memory) < batch_size: \n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=system, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=system)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = network(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(network.parameters(), 15)\n",
    "    optimiser.step()\n",
    "\n",
    "def actor_select_action(state):\n",
    "    global steps_passed\n",
    "    sample = random.random()\n",
    "    steps_passed += 1\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * steps_passed / e_decay)\n",
    "    if sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return network(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=system, dtype=torch.long)\n",
    "\n",
    "a2c_model = CriticNetwork(obs.shape, action_n).to(system)\n",
    "a2c_optimiser = opt.Adam(a2c_model.parameters(), lr=update)\n",
    "\n",
    "\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state.copy(), dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    probability_logs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    for i in range(1, 100):  # count():\n",
    "        action_probability, value = a2c_model(state)\n",
    "        action = torch.multinomial(action_probability, 1).item()\n",
    "        prob_log = torch.log(action_probability.squeeze(0)[action])\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = torch.FloatTensor(obs).unsqueeze(0).to(system)\n",
    "        probability_logs.append(prob_log)\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            episode_steps.append(i+1)\n",
    "            episode_rewards.append(total_reward)\n",
    "            print(total_reward)\n",
    "            break\n",
    "    \n",
    "\n",
    "    returns = []\n",
    "    discounted_return = 0\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_return = reward + gamma * discounted_return\n",
    "        returns.insert(0, discounted_return)\n",
    "    returns = torch.FloatTensor(returns, device=system)\n",
    "\n",
    "    probability_logs = torch.stack(probability_logs)\n",
    "    values = torch.cat(values).squeeze()\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss = -(probability_logs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "    a2c_optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    a2c_optimiser.step()\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "torch.save(a2c_model.state_dict(), 'mario_A2C.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
