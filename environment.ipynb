{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef65dca",
   "metadata": {},
   "source": [
    "# Super Mario Bros RL Notebook\n",
    "## Rohan H, Talaal S, Thang N, Yuet W\n",
    "### University of Bath\n",
    "\n",
    "![SegmentLocal](mario!.gif \"segment\")\n",
    "(we are going to have to delete him sadly...)\n",
    "\n",
    "\n",
    "### How to run:\n",
    "The official website for the game environment can be found here: https://pypi.org/project/gym-super-mario-bros/\n",
    "\n",
    "In a nutshell, you will need:\n",
    "- Python 3.5/3.6/3.7/3.8 (I have tested on 3.7)\n",
    "- gymnasium (gym is deprecated)\n",
    "- ipykernel for running the notebook\n",
    "- gym-super-mario-bros \n",
    "- other essential packages/libraries like NumPy\n",
    "- an average computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4068cc",
   "metadata": {},
   "source": [
    "### Check if the environment works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2bcfa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env_example = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='human')\n",
    "env_example = JoypadSpace(env_example, SIMPLE_MOVEMENT)\n",
    "\n",
    "test = False\n",
    "done = True\n",
    "if test:\n",
    "    for step in range(1000):\n",
    "        if done:\n",
    "            obs, info = env_example.reset()\n",
    "        obs, reward, terminated, truncated, info = env_example.step(env_example.action_space.sample())\n",
    "        done = not terminated or truncated\n",
    "\n",
    "    env_example.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d1c82",
   "metadata": {},
   "source": [
    "## Importing Libraries, Defining Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8df5bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "# Rohan's Cell\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# example here: https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers import FrameStack\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "import torch.nn.functional as func\n",
    "from torchvision import transforms as T\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "import itertools\n",
    "\n",
    "def device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "system = device()\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "    ['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, observations_n, actions_n):\n",
    "        super(DQN, self).__init__()\n",
    "        self.fully_connected = nn.Sequential(\n",
    "        nn.Linear(observations_n, 128),\n",
    "        nn.Linear(128, 64),\n",
    "        nn.Linear(64, actions_n)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        out = self.fully_connected(x).view(x.size(0), -1)\n",
    "        return out\n",
    "        \n",
    "\n",
    "class GreyScale(gym.ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(240, 256, 1),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "    def permutation(self, observation):\n",
    "        observation = np.transpose(observation, (2, 0, 1))\n",
    "        observation = torch.tensor(observation.copy(), dtype=torch.float)\n",
    "        return observation\n",
    "\n",
    "    def obs(self, obs):\n",
    "        obs = self.permutation(obs)\n",
    "        transform = T.Grayscale()\n",
    "        obs = transform(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='robot')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "env = GreyScale(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7b99c1b7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14520\\2169164032.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0maction_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mobservation_n\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;34m\"\"\"Resets the environment, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mobservation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;34m\"\"\"Returns a modified observation.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "batch_size = 50\n",
    "gamma = 0.9 # discount reward\n",
    "update = 0.05\n",
    "epsilon = 0.15 # exploration\n",
    "\n",
    "action_n = env.action_space.n\n",
    "obs, info = env.reset()\n",
    "observation_n = len(obs)\n",
    "\n",
    "policy_network = DQN(observation_n, action_n).to(system)\n",
    "target_network = DQN(observation_n, action_n).to(system)\n",
    "target_network.load_state_dict(policy_network.state_dict())\n",
    "\n",
    "optimiser = opt.AdamW(policy_network.parameters(), lr=update)\n",
    "memory = ReplayMemory(100) #increase to 10000 when training\n",
    "\n",
    "steps_passed = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_passed\n",
    "    sample = random.random()\n",
    "    steps_passed += 1\n",
    "    if sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return policy_network(state).max(1)[1].indices.view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=system, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba0889",
   "metadata": {},
   "source": [
    "## Model Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "876ad882",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_model():\n",
    "    if len(memory) < batch_size:\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=system, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = policy_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=system)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = policy_network(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(policy_network.parameters(), 15)\n",
    "    optimiser.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f0dd3",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e23763a5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14520\\2976186256.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mepisode_n\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtotal_reward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mreset\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;34m\"\"\"Resets the environment, returning a modified observation using :meth:`self.observation`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    379\u001b[0m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mobservation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    388\u001b[0m         \u001b[1;34m\"\"\"Returns a modified observation.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 389\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if system == torch.device(\"cuda\"):\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 30\n",
    "\n",
    "for episode_n in range(episodes):\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state.copy(), dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    for i in count():\n",
    "        action = select_action(state)\n",
    "        obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=system)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            #episode_durations.append(i+1)\n",
    "            #episode_rewards.append(total_reward)\n",
    "            break\n",
    "\n",
    "print('Done Training')\n",
    "print('Graph support not implemented yet')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
