{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef65dca",
   "metadata": {},
   "source": [
    "# Super Mario Bros RL Notebook\n",
    "## Rohan H, Talaal S, Thang N, Yuet W\n",
    "### University of Bath\n",
    "\n",
    "![SegmentLocal](mario!.gif \"segment\")\n",
    "(we are going to have to delete him sadly...)\n",
    "\n",
    "\n",
    "### How to run:\n",
    "The official website for the game environment can be found here: https://pypi.org/project/gym-super-mario-bros/\n",
    "\n",
    "In a nutshell, you will need:\n",
    "- Python 3.5/3.6/3.7/3.8 (I have tested on 3.7)\n",
    "- gymnasium (gym is deprecated)\n",
    "- ipykernel for running the notebook\n",
    "- gym-super-mario-bros \n",
    "- other essential packages/libraries like NumPy\n",
    "- an average computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4068cc",
   "metadata": {},
   "source": [
    "### Check if the environment works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcfa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env_example = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='human')\n",
    "env_example = JoypadSpace(env_example, SIMPLE_MOVEMENT)\n",
    "\n",
    "test = False\n",
    "done = True\n",
    "if test:\n",
    "    for step in range(1000):\n",
    "        if done:\n",
    "            obs, info = env_example.reset()\n",
    "        obs, reward, terminated, truncated, info = env_example.step(env_example.action_space.sample())\n",
    "        done = not terminated or truncated\n",
    "\n",
    "env_example.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d1c82",
   "metadata": {},
   "source": [
    "# /// DQN Algorithm ///\n",
    "\n",
    "## Importing Libraries, Defining Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8df5bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "# Rohan's Cell\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# example here: https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers.frame_stack import FrameStack\n",
    "from gym.core import ObservationWrapper\n",
    "import gym_super_mario_bros.actions as actions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "#import torch.nn.functional as func\n",
    "#from torchvision import transforms as T\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "#import itertools\n",
    "\n",
    "import cv2\n",
    "\n",
    "def clean_mem():\n",
    "    if torch.cuda.is_available():    \n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def device():\n",
    "    if torch.cuda.is_available():    \n",
    "        torch.cuda.empty_cache()\n",
    "        return torch.device(\"cuda\")\n",
    "    \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "system = device()\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "    ['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN_full(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQN_full, self).__init__()\n",
    "        self.convolution = nn.Sequential( # Convolutional layers\n",
    "        nn.Conv2d(shape[0], 16, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.fully_connected = nn.Sequential( # 32 neurons -> 64 neurons -> \"n\" actions \n",
    "            nn.Linear(output_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, actions_n)\n",
    "        )\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution(x).view(x.size(0), -1)\n",
    "        return self.fully_connected(out)\n",
    "    \n",
    "\n",
    "class ReShape(ObservationWrapper): # Put RGB channel as first argument\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            low=self.observation_space.low.min(),\n",
    "            high=self.observation_space.high.max(),\n",
    "            shape=(obs_shape[2], obs_shape[0], obs_shape[1]),\n",
    "            dtype=env.observation_space.dtype,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.transpose(observation, (2, 0, 1))\n",
    "\n",
    "class SmallBox84(ObservationWrapper): # Resize to 84x84\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=1.0,\n",
    "            shape=(1, 84, 84),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "    def observation(self, observation):\n",
    "        return cv2.resize(observation, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "class GreyScale(ObservationWrapper): # Convert to greyscale\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=1.0,\n",
    "            shape=(1, env.observation_space.shape[1], env.observation_space.shape[2]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "    def transform(self, observation):\n",
    "        return np.dot(observation[..., :3], [0.299, 0.587, 0.114]).reshape(1, observation.shape[0], observation.shape[1])\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.transform(observation)\n",
    "        return observation.astype(np.float32) / 255.0\n",
    "    \n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='robot')\n",
    "env = JoypadSpace(env, actions.SIMPLE_MOVEMENT)\n",
    "env = ReShape(env)\n",
    "env = GreyScale(env)\n",
    "env = SmallBox84(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b99c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95 # discount reward\n",
    "update = 0.001 # learning rate\n",
    "epsilon_start = 0.99 # initial exploration rate\n",
    "epsilon_end = 0.001 # final exploration rate\n",
    "e_decay = 2500 # decay rate\n",
    "action_n = env.action_space.n\n",
    "obs, info = env.reset()\n",
    "\n",
    "network = DQN_full(obs.shape, action_n).to(system)\n",
    "\n",
    "optimiser = opt.AdamW(network.parameters(), lr=update)\n",
    "memory = ReplayMemory(5000) \n",
    "steps_passed = 0\n",
    "\n",
    "def select_action(state, network):\n",
    "    global steps_passed\n",
    "    sample = random.random()\n",
    "    steps_passed += 1\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * steps_passed / e_decay)\n",
    "    if sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return network(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=system, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba0889",
   "metadata": {},
   "source": [
    "## Model Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "876ad882",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "clean_mem()\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size: # take 32 of 5000 to update model params\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=system, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=system)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = network(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(network.parameters(), 15)\n",
    "    optimiser.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f0dd3",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e23763a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  0\n",
      "504.0\n",
      "Starting Episode  1\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 227082240 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11168\\3821170045.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11168\\3096182179.py\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[0mreward_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mstate_action_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mnext_state_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11168\\644634875.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfully_connected\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 460\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 227082240 bytes."
     ]
    }
   ],
   "source": [
    "if system == torch.device(\"cuda\"):\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 15\n",
    "\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "\n",
    "clean_mem()\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state.copy(), dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = None\n",
    "    steps_passed = 0\n",
    "    for i in range(1000): # or use count() until terminated\n",
    "        action = select_action(state, network)\n",
    "        obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        reward = torch.tensor([reward], device=system)\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        if i % 5 == 0:\n",
    "            optimize_model()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clean_mem()\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    clean_mem()\n",
    "        \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(total_reward)\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "if system == torch.device(\"cuda\"):\n",
    "    torch.save(network.state_dict(), 'mario_DQN_full.pth')\n",
    "else:\n",
    "    torch.save(network.state_dict(), 'mario_DQN_short.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "52f5554c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup ready for DDQN\n",
    "del memory, optimiser, network\n",
    "clean_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561a54e",
   "metadata": {},
   "source": [
    "## /// DDQN ///\n",
    "\n",
    "Since it is a variant of the DQN algorithm, some of the assets remain the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9024e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(5000)\n",
    "\n",
    "class DDQN(nn.Module):\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.convolution = nn.Sequential( # Convolutional layers\n",
    "        nn.Conv2d(shape[0], 16, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.fully_connected = nn.Sequential( # 32 neurons -> 64 neurons -> \"n\" actions \n",
    "            nn.Linear(output_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, actions_n)\n",
    "        )\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution(x).view(x.size(0), -1)\n",
    "        return self.fully_connected(out\n",
    "        )\n",
    "\n",
    "primary_network = DQN_full(obs.shape, action_n).to(system) \n",
    "target_network = DDQN(obs.shape, action_n).to(system)\n",
    "target_network.load_state_dict(primary_network.state_dict())\n",
    "target_network.eval()\n",
    "\n",
    "optimiser = opt.AdamW(primary_network.parameters(), lr=update)\n",
    "\n",
    "def optimize_ddqn():\n",
    "    if len(memory) < batch_size: # take 32 of 5000 to update model params\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=system, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = primary_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=system)\n",
    "    with torch.no_grad():\n",
    "        if len(non_final_next_states) > 0:\n",
    "            # Select best actions using the primary network\n",
    "            q_values_primary = primary_network(non_final_next_states)\n",
    "            best_actions = q_values_primary.argmax(1).unsqueeze(1)\n",
    "\n",
    "            q_values_target = target_network(non_final_next_states)\n",
    "            next_state_values[non_final_mask] = q_values_target.gather(1, best_actions).squeeze(1)\n",
    "\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(primary_network.parameters(), 15)\n",
    "    optimiser.step()\n",
    "\n",
    "\n",
    "def optimise_target_network(tau=0.001):\n",
    "    for target_param, primary_param in zip(target_network.parameters(), primary_network.parameters()):\n",
    "        target_param.data.copy_(tau * primary_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff799a4",
   "metadata": {},
   "source": [
    "### Training Loop for DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3668ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  0\n",
      "113.0\n",
      "Starting Episode  1\n",
      "87.0\n",
      "Starting Episode  2\n",
      "122.0\n",
      "Starting Episode  3\n",
      "106.0\n",
      "Starting Episode  4\n",
      "127.0\n",
      "Starting Episode  5\n",
      "114.0\n",
      "Starting Episode  6\n",
      "134.0\n",
      "Starting Episode  7\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "[enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2416640 bytes.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11168\\1240796454.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0moptimize_ddqn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m             \u001b[0moptimise_target_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtau\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11168\\284168284.py\u001b[0m in \u001b[0;36moptimize_ddqn\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m             \u001b[0mbest_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_values_primary\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m             \u001b[0mq_values_target\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnon_final_next_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m             \u001b[0mnext_state_values\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnon_final_mask\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mq_values_target\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbest_actions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11168\\284168284.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     27\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 28\u001b[1;33m         \u001b[0mout\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvolution\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mview\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     29\u001b[0m         return self.fully_connected(out\n\u001b[0;32m     30\u001b[0m         )\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\container.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 204\u001b[1;33m             \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    205\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[0;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1194\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1195\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m \u001b[1;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\nn\\modules\\conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m    459\u001b[0m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[1;32m--> 460\u001b[1;33m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[0;32m    461\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\c10\\core\\impl\\alloc_cpu.cpp:72] data. DefaultCPUAllocator: not enough memory: you tried to allocate 2416640 bytes."
     ]
    }
   ],
   "source": [
    "if system == torch.device(\"cuda\"):\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 15\n",
    "\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "clean_mem()\n",
    "\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state.copy(), dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = None\n",
    "    steps_passed = 0\n",
    "    for i in range(100): # or use count() to go until terminated\n",
    "        action = select_action(state, primary_network)\n",
    "        obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        reward = torch.tensor([reward], device=system)\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        if i % 5 == 0:\n",
    "            optimize_ddqn()\n",
    "            optimise_target_network(tau=0.001)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clean_mem()\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(total_reward)\n",
    "\n",
    "    clean_mem()\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "if system == torch.device(\"cuda\"):\n",
    "    torch.save(primary_network.state_dict(), 'mario_DDQN_full.pth')\n",
    "else:\n",
    "    torch.save(primary_network.state_dict(), 'mario_DDQN_short.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "94266e15",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'primary_network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_11168\\2008042815.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Cleanup ready for A2C\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mprimary_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_network\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mclean_mem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'primary_network' is not defined"
     ]
    }
   ],
   "source": [
    "# Cleanup ready for A2C\n",
    "del primary_network, target_network, optimiser, memory \n",
    "clean_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f2571",
   "metadata": {},
   "source": [
    "# /// A2C Algorithm ///\n",
    "\n",
    "To save time, we will use some of the same classes and functions from DQN. Make sure you run all of the prior code (except the training loop for DQN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "31612832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  0\n",
      "757.0\n",
      "Starting Episode  1\n",
      "504.0\n",
      "Starting Episode  2\n",
      "504.0\n",
      "Starting Episode  3\n",
      "-58.0\n",
      "Starting Episode  4\n",
      "-46.0\n",
      "Starting Episode  5\n",
      "-50.0\n",
      "Starting Episode  6\n",
      "-50.0\n",
      "Starting Episode  7\n",
      "-50.0\n",
      "Starting Episode  8\n",
      "-50.0\n",
      "Starting Episode  9\n",
      "-46.0\n",
      "Starting Episode  10\n",
      "52.0\n",
      "Starting Episode  11\n",
      "776.0\n",
      "Starting Episode  12\n",
      "504.0\n",
      "Starting Episode  13\n",
      "744.0\n",
      "Starting Episode  14\n",
      "504.0\n",
      "Starting Episode  15\n",
      "504.0\n",
      "Starting Episode  16\n",
      "507.0\n",
      "Starting Episode  17\n",
      "507.0\n",
      "Starting Episode  18\n",
      "241.0\n",
      "Starting Episode  19\n",
      "-11.0\n",
      "Starting Episode  20\n",
      "-32.0\n",
      "Starting Episode  21\n",
      "-40.0\n",
      "Starting Episode  22\n",
      "-44.0\n",
      "Starting Episode  23\n",
      "-53.0\n",
      "Starting Episode  24\n",
      "-45.0\n",
      "Starting Episode  25\n",
      "-41.0\n",
      "Starting Episode  26\n",
      "-25.0\n",
      "Starting Episode  27\n",
      "43.0\n",
      "Starting Episode  28\n",
      "632.0\n",
      "Starting Episode  29\n",
      "504.0\n",
      "Starting Episode  30\n",
      "504.0\n",
      "Starting Episode  31\n",
      "504.0\n",
      "Starting Episode  32\n",
      "504.0\n",
      "Starting Episode  33\n",
      "743.0\n",
      "Starting Episode  34\n",
      "504.0\n",
      "Starting Episode  35\n",
      "504.0\n",
      "Starting Episode  36\n",
      "631.0\n",
      "Starting Episode  37\n",
      "504.0\n",
      "Starting Episode  38\n",
      "503.0\n",
      "Starting Episode  39\n",
      "632.0\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "steps_passed = 0\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "gamma = 0.99\n",
    "a2c_update = 0.001\n",
    "\n",
    "if system == torch.device(\"cuda\"):\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 40\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.convolution = nn.Sequential( # combined convolutional layers\n",
    "        nn.Conv2d(shape[0],16, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.actor = nn.Sequential(  \n",
    "            nn.Linear(output_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, actions_n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(output_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def get_values(self, x): # for debugging\n",
    "        x = self.convolution(x)\n",
    "        actor = self.actor(x).view(x.size(0), -1)\n",
    "        critic = self.critic(x).view(x.size(0), -1)\n",
    "        return actor, critic\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x).view(x.size(0), -1)\n",
    "        probs = nn.Softmax(dim=1)(self.actor(x))\n",
    "        state_values = self.critic(x)\n",
    "        return probs, state_values\n",
    "\n",
    "\n",
    "a2c_model = CriticNetwork(obs.shape, action_n).to(system)\n",
    "a2c_optimiser = opt.Adam(a2c_model.parameters(), lr=a2c_update)\n",
    "\n",
    "clean_mem()\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state.copy(), dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    probability_logs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    for i in range(1000):  # or count():\n",
    "        action_probability, value = a2c_model(state)\n",
    "        action = torch.multinomial(action_probability, 1).item()\n",
    "        prob_log = torch.log(action_probability.squeeze(0)[action])\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "        probability_logs.append(prob_log)\n",
    "        total_reward += reward\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "        state = next_state\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clean_mem()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_steps.append(i+1)\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(total_reward)\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    discounted_return = 0\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_return = reward + gamma * discounted_return\n",
    "        returns.insert(0, discounted_return)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=system)\n",
    "\n",
    "    probability_logs = torch.stack(probability_logs)\n",
    "    values = torch.cat(values).squeeze()\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss = -(probability_logs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "    a2c_optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(a2c_model.parameters(), 0.5)\n",
    "    a2c_optimiser.step()\n",
    "\n",
    "    clean_mem()\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "if system == torch.device(\"cuda\"):\n",
    "    torch.save(a2c_model.state_dict(), 'mario_A2C_full.pth')\n",
    "else:\n",
    "    torch.save(a2c_model.state_dict(), 'mario_A2C_short.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
