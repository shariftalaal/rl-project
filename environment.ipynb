{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef65dca",
   "metadata": {},
   "source": [
    "# Super Mario Bros RL Notebook\n",
    "## Rohan H, Talaal S, Thang N, Yuet W\n",
    "### University of Bath\n",
    "\n",
    "![SegmentLocal](mario!.gif \"segment\")\n",
    "(we are going to have to delete him sadly...)\n",
    "\n",
    "\n",
    "### How to run:\n",
    "The official website for the game environment can be found here: https://pypi.org/project/gym-super-mario-bros/\n",
    "\n",
    "In a nutshell, you will need:\n",
    "- Python 3.5/3.6/3.7/3.8 (I have tested on 3.7)\n",
    "- gymnasium (gym is deprecated)\n",
    "- ipykernel for running the notebook\n",
    "- gym-super-mario-bros \n",
    "- other essential packages/libraries like NumPy\n",
    "- an average computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4068cc",
   "metadata": {},
   "source": [
    "### Check if the environment works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcfa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env_example = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='human')\n",
    "env_example = JoypadSpace(env_example, SIMPLE_MOVEMENT)\n",
    "\n",
    "test = False\n",
    "done = True\n",
    "if test:\n",
    "    for step in range(1000):\n",
    "        if done:\n",
    "            obs, info = env_example.reset()\n",
    "        obs, reward, terminated, truncated, info = env_example.step(env_example.action_space.sample())\n",
    "        done = not terminated or truncated\n",
    "\n",
    "env_example.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d1c82",
   "metadata": {},
   "source": [
    "# /// DQN Algorithm ///\n",
    "\n",
    "## Importing Libraries, Defining Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "# Rohan's Cell\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# example here: https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import math\n",
    "import gc\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers.frame_stack import FrameStack\n",
    "from gym.core import ObservationWrapper\n",
    "import gym_super_mario_bros.actions as actions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "#import torch.nn.functional as func\n",
    "#from torchvision import transforms as T\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "#import itertools\n",
    "\n",
    "import cv2\n",
    "\n",
    "def clean_mem():\n",
    "    if torch.cuda.is_available():    \n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "\n",
    "def cuda():\n",
    "    if torch.cuda.is_available():\n",
    "        return True\n",
    "    else: \n",
    "        return False\n",
    "    \n",
    "def device():\n",
    "    if torch.cuda.is_available():    \n",
    "        torch.cuda.empty_cache()\n",
    "        return torch.device(\"cuda\")\n",
    "    \n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "system = device()\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "    ['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN_full(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQN_full, self).__init__()\n",
    "        self.convolution = nn.Sequential( # Convolutional layers\n",
    "        nn.Conv2d(shape[0], 32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.fully_connected = nn.Sequential( # 32 neurons -> 64 neurons -> \"n\" actions \n",
    "            nn.Linear(output_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, actions_n)\n",
    "        )\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution(x).view(x.size(0), -1)\n",
    "        return self.fully_connected(out)\n",
    "    \n",
    "\n",
    "class Transpose(ObservationWrapper): # Put Greyscale channel as first argument\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            low=self.observation_space.low.min(),\n",
    "            high=self.observation_space.high.max(),\n",
    "            shape=(obs_shape[2], obs_shape[0], obs_shape[1]),\n",
    "            dtype=env.observation_space.dtype,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.transpose(observation, (2, 0, 1))\n",
    "\n",
    "class SmallBox84(ObservationWrapper): # Resize to 84x84\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(84, 84, obs_shape[2]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "    def observation(self, observation):\n",
    "        observation = cv2.resize(observation, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "        if observation.ndim == 2:\n",
    "            observation = observation[:, :, np.newaxis]\n",
    "        return observation\n",
    "\n",
    "class GreyScale(ObservationWrapper): # Convert to greyscale\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=255,\n",
    "            shape=(obs_shape[0], obs_shape[1], 1),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "    def transform(self, observation):\n",
    "        return np.dot(observation[..., :3], [0.299, 0.587, 0.114])\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.transform(observation)\n",
    "        observation = observation.astype(np.uint8)\n",
    "        observation = np.expand_dims(observation, axis=-1)\n",
    "        return observation\n",
    "\n",
    "class Normalise(ObservationWrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            low=0.0,\n",
    "            high=1.0,\n",
    "            shape=env.observation_space.shape,\n",
    "            dtype=np.float32\n",
    "        )\n",
    "    def observation(self, observation):\n",
    "        return observation.astype(np.float32) / 255.0\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='robot')\n",
    "env = JoypadSpace(env, actions.SIMPLE_MOVEMENT)\n",
    "env = GreyScale(env)\n",
    "env = SmallBox84(env)\n",
    "env = Transpose(env)\n",
    "env = Normalise(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7b99c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.99 # discount reward\n",
    "update = 0.0001 # learning rate\n",
    "epsilon_start = 0.9 # initial exploration rate\n",
    "epsilon_end = 0.01 # final exploration rate\n",
    "max_steps = 1000 # step limit for decay\n",
    "action_n = env.action_space.n\n",
    "obs, info = env.reset()\n",
    "eps_decay = True\n",
    "\n",
    "network = DQN_full(obs.shape, action_n).to(system)\n",
    "\n",
    "optimiser = opt.AdamW(network.parameters(), lr=update)\n",
    "memory = ReplayMemory(5000) \n",
    "steps_passed = 0\n",
    "\n",
    "def get_eps():\n",
    "    if steps_passed >= max_steps:\n",
    "        return epsilon_end\n",
    "    else:\n",
    "        return epsilon_start - (epsilon_start - epsilon_end) * (steps_passed / max_steps)\n",
    "\n",
    "epsilon = 0.1\n",
    "def select_action(state, network):\n",
    "    global steps_passed\n",
    "    sample = random.random()\n",
    "    steps_passed += 1\n",
    "    if eps_decay:\n",
    "        epsilon = get_eps() \n",
    "    if sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return network(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=system, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba0889",
   "metadata": {},
   "source": [
    "## Model Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "876ad882",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "clean_mem()\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size: # take 32 of 5000 to update model params\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=system, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=system)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = network(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(network.parameters(), 15)\n",
    "    optimiser.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f0dd3",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e23763a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  1\n",
      "1437.0\n",
      "Starting Episode  2\n",
      "1851.0\n",
      "Starting Episode  3\n",
      "1544.0\n",
      "Starting Episode  4\n",
      "2656.0\n",
      "Starting Episode  5\n",
      "1505.0\n",
      "Starting Episode  6\n",
      "1722.0\n",
      "Starting Episode  7\n",
      "668.0\n",
      "Starting Episode  8\n",
      "1833.0\n",
      "Starting Episode  9\n",
      "1622.0\n",
      "Starting Episode  10\n",
      "1842.0\n",
      "Starting Episode  11\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14936\\2857693229.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m5\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m             \u001b[0moptimize_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m100\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_14936\\3096182179.py\u001b[0m in \u001b[0;36moptimize_model\u001b[1;34m()\u001b[0m\n\u001b[0;32m     28\u001b[0m     \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_action_values\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpected_state_action_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m     \u001b[0moptimiser\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m     \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mzero_grad\u001b[1;34m(self, set_to_none)\u001b[0m\n\u001b[0;32m    277\u001b[0m                                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrequires_grad_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    278\u001b[0m                             \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mforeach\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_sparse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 279\u001b[1;33m                                 \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    280\u001b[0m                             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    281\u001b[0m                                 \u001b[0mper_device_and_dtype_grads\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if cuda():\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 30\n",
    "\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "\n",
    "clean_mem()\n",
    "\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n+1)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = None\n",
    "    steps_passed = 0\n",
    "    for i in range(20000): # or use count() until terminated\n",
    "        action = select_action(state, network)\n",
    "        obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        reward = torch.tensor([reward], device=system)\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        if i % 5 == 0:\n",
    "            optimize_model()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clean_mem()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    clean_mem()\n",
    "        \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(total_reward)\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "if cuda():\n",
    "    torch.save(network.state_dict(), 'mario_DQN_full.pth')\n",
    "else:\n",
    "    torch.save(network.state_dict(), 'mario_DQN_short.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "52f5554c",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'network' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_280\\4095647527.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Cleanup ready for DDQN\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0mmemory\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moptimiser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mclean_mem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'network' is not defined"
     ]
    }
   ],
   "source": [
    "# Cleanup ready for DDQN\n",
    "del memory, optimiser, network\n",
    "clean_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a561a54e",
   "metadata": {},
   "source": [
    "## /// DDQN ///\n",
    "\n",
    "Since it is a variant of the DQN algorithm, some of the assets remain the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9024e282",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = ReplayMemory(5000)\n",
    "\n",
    "class DDQN(nn.Module): # same as DQN class\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DDQN, self).__init__()\n",
    "        self.convolution = nn.Sequential( \n",
    "        nn.Conv2d(shape[0], 32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.fully_connected = nn.Sequential( \n",
    "            nn.Linear(output_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, actions_n)\n",
    "        )\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution(x).view(x.size(0), -1)\n",
    "        return self.fully_connected(out\n",
    "        )\n",
    "\n",
    "primary_network = DQN_full(obs.shape, action_n).to(system) \n",
    "target_network = DDQN(obs.shape, action_n).to(system)\n",
    "target_network.load_state_dict(primary_network.state_dict())\n",
    "target_network.eval()\n",
    "\n",
    "optimiser = opt.AdamW(primary_network.parameters(), lr=update)\n",
    "\n",
    "def optimize_ddqn():\n",
    "    if len(memory) < batch_size: \n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=system, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = primary_network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=system)\n",
    "    with torch.no_grad():\n",
    "        if len(non_final_next_states) > 0:\n",
    "            # Select best actions using the primary network\n",
    "            q_values_primary = primary_network(non_final_next_states)\n",
    "            best_actions = q_values_primary.argmax(1).unsqueeze(1)\n",
    "\n",
    "            q_values_target = target_network(non_final_next_states)\n",
    "            next_state_values[non_final_mask] = q_values_target.gather(1, best_actions).squeeze(1)\n",
    "\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(primary_network.parameters(), 15)\n",
    "    optimiser.step()\n",
    "\n",
    "\n",
    "def optimise_target_network(tau=0.001):\n",
    "    for target_param, primary_param in zip(target_network.parameters(), primary_network.parameters()):\n",
    "        target_param.data.copy_(tau * primary_param.data + (1.0 - tau) * target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff799a4",
   "metadata": {},
   "source": [
    "### Training Loop for DDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3668ecbb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  1\n",
      "1660.0\n",
      "Starting Episode  2\n",
      "1468.0\n",
      "Starting Episode  3\n",
      "1179.0\n",
      "Starting Episode  4\n",
      "1555.0\n",
      "Starting Episode  5\n",
      "1574.0\n",
      "Starting Episode  6\n",
      "2017.0\n",
      "Starting Episode  7\n",
      "2446.0\n",
      "Starting Episode  8\n",
      "2387.0\n",
      "Starting Episode  9\n",
      "1866.0\n",
      "Starting Episode  10\n",
      "1575.0\n",
      "Starting Episode  11\n",
      "1508.0\n",
      "Starting Episode  12\n",
      "1712.0\n",
      "Starting Episode  13\n",
      "3243.0\n",
      "Starting Episode  14\n",
      "2341.0\n",
      "Starting Episode  15\n",
      "2002.0\n",
      "Starting Episode  16\n",
      "1713.0\n",
      "Starting Episode  17\n",
      "1469.0\n",
      "Starting Episode  18\n",
      "1399.0\n",
      "Starting Episode  19\n",
      "1957.0\n",
      "Starting Episode  20\n",
      "1484.0\n",
      "Starting Episode  21\n",
      "2606.0\n",
      "Starting Episode  22\n",
      "2157.0\n",
      "Starting Episode  23\n",
      "1558.0\n",
      "Starting Episode  24\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_280\\703735732.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# or use count() to go until terminated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprimary_network\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0mtotal_reward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mreward\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 384\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    385\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    383\u001b[0m         \u001b[1;34m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    384\u001b[0m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 385\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    386\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    387\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_280\\2089446953.py\u001b[0m in \u001b[0;36mobservation\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 144\u001b[1;33m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    145\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0mobservation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_280\\2089446953.py\u001b[0m in \u001b[0;36mtransform\u001b[1;34m(self, observation)\u001b[0m\n\u001b[0;32m    139\u001b[0m         )\n\u001b[0;32m    140\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m...\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0.299\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.587\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.114\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobservation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mdot\u001b[1;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if cuda():\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 30\n",
    "\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "clean_mem()\n",
    "\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n+1)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = None\n",
    "    steps_passed = 0\n",
    "    for i in range(20000): # or use count() to go until terminated\n",
    "        action = select_action(state, primary_network)\n",
    "        obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        total_reward += reward\n",
    "        reward = torch.tensor([reward], device=system)\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "        if i % 5 == 0:\n",
    "            optimize_ddqn()\n",
    "            optimise_target_network(tau=0.001)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clean_mem()\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(total_reward)\n",
    "\n",
    "    clean_mem()\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "if system == torch.device(\"cuda\"):\n",
    "    torch.save(primary_network.state_dict(), 'mario_DDQN_full.pth')\n",
    "else:\n",
    "    torch.save(primary_network.state_dict(), 'mario_DDQN_short.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94266e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup ready for A2C\n",
    "del primary_network, target_network, optimiser, memory, episodes\n",
    "clean_mem()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f2571",
   "metadata": {},
   "source": [
    "# /// A2C Algorithm ///\n",
    "\n",
    "To save time, we will use some of the same classes and functions from DQN. Make sure you run all of the prior code (except the training loop for DQN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31612832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  1\n",
      "610.0\n",
      "Starting Episode  2\n",
      "493.0\n"
     ]
    }
   ],
   "source": [
    "steps_passed = 0\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "gamma = 0.99\n",
    "a2c_update = 0.001\n",
    "\n",
    "if system == torch.device(\"cuda\"):\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 30\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.convolution = nn.Sequential( # combined convolutional layers\n",
    "        nn.Conv2d(shape[0],32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.actor = nn.Sequential(  \n",
    "            nn.Linear(output_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, actions_n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(output_size, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def get_values(self, x): # for debugging\n",
    "        x = self.convolution(x)\n",
    "        actor = self.actor(x).view(x.size(0), -1)\n",
    "        critic = self.critic(x).view(x.size(0), -1)\n",
    "        return actor, critic\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x).view(x.size(0), -1)\n",
    "        probs = nn.Softmax(dim=1)(self.actor(x))\n",
    "        state_values = self.critic(x)\n",
    "        return probs, state_values\n",
    "\n",
    "\n",
    "a2c_model = CriticNetwork(obs.shape, action_n).to(system)\n",
    "a2c_optimiser = opt.Adam(a2c_model.parameters(), lr=a2c_update)\n",
    "\n",
    "clean_mem()\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n+1)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    probability_logs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    for i in range(20000):  # or count():\n",
    "        action_probability, value = a2c_model(state)\n",
    "        action = torch.multinomial(action_probability, 1).item()\n",
    "        prob_log = torch.log(action_probability.squeeze(0)[action])\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "        probability_logs.append(prob_log)\n",
    "        total_reward += reward\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "        state = next_state\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            clean_mem()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "\n",
    "    episode_steps.append(i+1)\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(total_reward)\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    discounted_return = 0\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_return = reward + gamma * discounted_return\n",
    "        returns.insert(0, discounted_return)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=system)\n",
    "\n",
    "    probability_logs = torch.stack(probability_logs)\n",
    "    values = torch.cat(values).squeeze()\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss = -(probability_logs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "    a2c_optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(a2c_model.parameters(), 0.5)\n",
    "    a2c_optimiser.step()\n",
    "\n",
    "    clean_mem()\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "if system == torch.device(\"cuda\"):\n",
    "    torch.save(a2c_model.state_dict(), 'mario_A2C_full.pth')\n",
    "else:\n",
    "    torch.save(a2c_model.state_dict(), 'mario_A2C_short.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "953a0ae3",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'a2c_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_13332\\3149658247.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mdel\u001b[0m \u001b[0ma2c_model\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma2c_optimiser\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0ma2c_update\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepisodes\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'a2c_model' is not defined"
     ]
    }
   ],
   "source": [
    "env.close()\n",
    "del a2c_model, a2c_optimiser, a2c_update, episodes, env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b939ca",
   "metadata": {},
   "source": [
    "## Run each model on the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38fd05fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(84, 84, 240)\n"
     ]
    }
   ],
   "source": [
    "env = env = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='human')\n",
    "env = JoypadSpace(env, actions.SIMPLE_MOVEMENT)\n",
    "env = GreyScale(env)\n",
    "env = SmallBox84(env)\n",
    "env = Transpose(env)\n",
    "env = Normalise(env)\n",
    "obs, info = env.reset()\n",
    "\n",
    "dqn = DQN_full(obs.shape, env.action_space.n).to(system)\n",
    "if cuda():\n",
    "    loaded = torch.load(\"mario_DQN_full.pth\", map_location=system)\n",
    "    if isinstance(loaded, dict) and 'model_state_dict' in loaded:\n",
    "        dqn.load_state_dict(loaded['model_state_dict'])\n",
    "    else:\n",
    "        dqn.load_state_dict(loaded)\n",
    "else:\n",
    "    loaded = torch.load(\"mario_DQN_short.pth\", map_location=system)\n",
    "    if isinstance(loaded, dict) and 'model_state_dict' in loaded:\n",
    "        dqn.load_state_dict(loaded['model_state_dict'])\n",
    "    else:\n",
    "        dqn.load_state_dict(loaded)\n",
    "dqn.eval()\n",
    "\n",
    "done = None\n",
    "state, info = env.reset()\n",
    "'''\n",
    "if True:\n",
    "    for step in range(10000):\n",
    "        state = torch.tensor(state, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            q_values = dqn(state)\n",
    "            action = q_values.argmax().item()\n",
    "        next_state, reward, terminated, truncated, info = env.step(action)\n",
    "        done = terminated or truncated\n",
    "\n",
    "        state = next_state\n",
    "        if done:\n",
    "            break\n",
    "'''\n",
    "env.close()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
