{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef65dca",
   "metadata": {},
   "source": [
    "# Super Mario Bros RL Notebook\n",
    "## Rohan H, Talaal S, Thang N, Yuet W\n",
    "### University of Bath\n",
    "\n",
    "![SegmentLocal](mario!.gif \"segment\")\n",
    "(we are going to have to delete him sadly...)\n",
    "\n",
    "\n",
    "### How to run:\n",
    "The official website for the game environment can be found here: https://pypi.org/project/gym-super-mario-bros/\n",
    "\n",
    "In a nutshell, you will need:\n",
    "- Python 3.5/3.6/3.7/3.8 (I have tested on 3.7)\n",
    "- gymnasium (gym is deprecated)\n",
    "- ipykernel for running the notebook\n",
    "- gym-super-mario-bros \n",
    "- other essential packages/libraries like NumPy\n",
    "- an average computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2bcfa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "c:\\Users\\Rohan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\Rohan\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import time\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env_example = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='human')\n",
    "env_example = JoypadSpace(env_example, SIMPLE_MOVEMENT)\n",
    "\n",
    "def env_run(env, steps):\n",
    "    done = True\n",
    "    for step in range(steps):\n",
    "        if done:\n",
    "            obs, info = env.reset()\n",
    "        obs, reward, terminated, truncated, info = env.step(env.action_space.sample())\n",
    "        done = terminated or truncated\n",
    "        time.sleep(0.01)\n",
    "    env.close()\n",
    "\n",
    "env_run(env_example, steps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96786cb",
   "metadata": {},
   "source": [
    "## Independent Work\n",
    "\n",
    "Each of the cells below will belong to each member of the group. If you have independent work to be getting on with, you should do it in one of the cells below. Branches are yet to be organised, either by member or by task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f26537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Talaal's Cell\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rohan's Cell\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import random\n",
    "\n",
    "class MCAgent:\n",
    "    \n",
    "    def __init__(self, env, epsilon = 0.15, gamma = 0.9) :\n",
    "\n",
    "        self.env = env\n",
    "        self.returns = {}\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "        self.q_values = {}\n",
    "    \n",
    "    def state(self, state):\n",
    "        if isinstance(state, np.ndarray):\n",
    "            return tuple(state.flatten())\n",
    "        return state\n",
    "    \n",
    "    def policy(self, state, greedy = False) :\n",
    "        state = self.state(state)\n",
    "        available_actions = list(range(self.env.action_space.n))\n",
    "        \n",
    "        # Random tie-breaking\n",
    "        random.shuffle(available_actions)\n",
    "        \n",
    "        # Take random action.\n",
    "        if (greedy == False and random.random() < self.epsilon) :\n",
    "            return random.choice(available_actions)\n",
    "        # Take greedy action.\n",
    "        else :\n",
    "            q_table = {(state, a) : mean(self.returns.get((state, a), [0])) for a in available_actions}\n",
    "            q_values = [q_table.get((state, a), 0) for a in available_actions]\n",
    "            return available_actions[q_values.index(max(q_values))]\n",
    "        \n",
    "    def learn(self, state_action_pairs, rewards, next_states) :\n",
    "        \n",
    "        episode_return = 0\n",
    "        \n",
    "        # Loop through our episode experience (backwards, as per the pseudocode).\n",
    "        while(len(state_action_pairs) > 0) :\n",
    "            \n",
    "            # We \"pop\" the last time-step from our experience lists\n",
    "            # each iteration until they are empty.\n",
    "            state, action = state_action_pairs.pop()\n",
    "            reward = rewards.pop()\n",
    "            next_state = next_states.pop()\n",
    "            \n",
    "            # Update the return earned after this time-step.\n",
    "            episode_return = reward + self.gamma * episode_return\n",
    "            \n",
    "            # If this is our first-visit to this state-action pair in this\n",
    "            # episode, update its list of returns.\n",
    "            if (not (state, action) in state_action_pairs) :\n",
    "                returns_list = self.returns.get((state, action), [])\n",
    "                returns_list.append(episode_return)\n",
    "                self.returns[(state, action)] = returns_list.copy()\n",
    "            \n",
    "    def generate_episode(self) :\n",
    "        \n",
    "        # Initialise variables for storing our agent's experience.\n",
    "        state_action_pairs = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        summed_rewards = 0\n",
    "        \n",
    "        # Initialise environment.\n",
    "        state, info = self.env.reset()\n",
    "        state = self.state(state)\n",
    "        terminal = False\n",
    "        limit = 1000 # Prevent infinite episodes.\n",
    "        \n",
    "        # Generate a full episode of experience.\n",
    "        while (not terminal) or (limit > 0) :\n",
    "            action = self.policy(state)\n",
    "            next_state, reward, terminated, truncated, info = self.env.step(action)\n",
    "            terminal = terminated or truncated\n",
    "            \n",
    "            next_state = self.state(next_state)\n",
    "            state_action_pairs.append((state, action))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            summed_rewards += reward\n",
    "            limit -= 1\n",
    "                \n",
    "        return state_action_pairs, rewards, next_states, summed_rewards\n",
    "    \n",
    "\n",
    "'''\n",
    "num_agents = 4\n",
    "num_episodes = 50\n",
    "mc_rewards = []\n",
    "\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v3', apply_api_compatibility=True, render_mode='robot')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "for agent in range(0, num_agents) :\n",
    "    #print(\"Agent {}\".format(agent + 1))\n",
    "    episode_rewards = []\n",
    "\n",
    "    mcagent = MCAgent(env)\n",
    "    \n",
    "    for episode in range(0, num_episodes) :\n",
    "\n",
    "        # Update cumulative reward for episode.\n",
    "        state_action_pairs, rewards, next_states, sum_rewards = mcagent.generate_episode()\n",
    "        mcagent.learn(state_action_pairs, rewards, next_states)\n",
    "\n",
    "        episode_rewards.append(sum_rewards)\n",
    "    mc_rewards.append(episode_rewards)\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thang's Cell\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yuet's Cell\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
