{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef65dca",
   "metadata": {},
   "source": [
    "# Super Mario Bros RL Notebook\n",
    "## Rohan H, Talaal S, Thang N, Yuet W\n",
    "### University of Bath\n",
    "\n",
    "![SegmentLocal](mario!.gif \"segment\")\n",
    "(we are going to have to delete him sadly...)\n",
    "\n",
    "\n",
    "### How to run:\n",
    "The official website for the game environment can be found here: https://pypi.org/project/gym-super-mario-bros/\n",
    "\n",
    "In a nutshell, you will need:\n",
    "- Python 3.5/3.6/3.7/3.8 (I have tested on 3.7)\n",
    "- gymnasium (gym is deprecated)\n",
    "- ipykernel for running the notebook\n",
    "- gym-super-mario-bros \n",
    "- other essential packages/libraries like NumPy\n",
    "- an average computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c4068cc",
   "metadata": {},
   "source": [
    "### Check if the environment works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcfa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\user\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env_example = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='human')\n",
    "env_example = JoypadSpace(env_example, SIMPLE_MOVEMENT)\n",
    "\n",
    "test = False\n",
    "done = True\n",
    "if test:\n",
    "    for step in range(1000):\n",
    "        if done:\n",
    "            obs, info = env_example.reset()\n",
    "        obs, reward, terminated, truncated, info = env_example.step(env_example.action_space.sample())\n",
    "        done = not terminated or truncated\n",
    "\n",
    "env_example.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd6d1c82",
   "metadata": {},
   "source": [
    "# /// DQN Algorithm ///\n",
    "\n",
    "## Importing Libraries, Defining Functions and Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8df5bad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rohan's Cell\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# example here: https://docs.pytorch.org/tutorials/intermediate/reinforcement_q_learning.html\n",
    "\n",
    "import math\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym.spaces import Box\n",
    "from gym.wrappers.frame_stack import FrameStack\n",
    "from gym.core import ObservationWrapper\n",
    "import gym_super_mario_bros.actions as actions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as opt\n",
    "#import torch.nn.functional as func\n",
    "#from torchvision import transforms as T\n",
    "\n",
    "from collections import namedtuple, deque\n",
    "#from itertools import count\n",
    "#import itertools\n",
    "\n",
    "import cv2\n",
    "\n",
    "def device():\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device(\"cuda\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "system = device()\n",
    "\n",
    "Transition = namedtuple('Transition',\n",
    "    ['state', 'action', 'next_state', 'reward'])\n",
    "\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.memory = deque([], maxlen=capacity)\n",
    "\n",
    "    def push(self, *args):\n",
    "        self.memory.append(Transition(*args))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "\n",
    "\n",
    "class DQN_full(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(DQN_full, self).__init__()\n",
    "        self.convolution = nn.Sequential( # Convolutional layers\n",
    "        nn.Conv2d(shape[0], 16, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(16, 32, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 32, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.fully_connected = nn.Sequential( # 64 neurons -> 512 neurons -> \"n\" actions \n",
    "            nn.Linear(output_size, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, actions_n)\n",
    "        )\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.convolution(x).view(x.size(0), -1)\n",
    "        return self.fully_connected(out)\n",
    "    \n",
    "\n",
    "class ReShape(ObservationWrapper): # Put RGB channel as first argument\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        obs_shape = env.observation_space.shape\n",
    "        self.observation_space = Box(\n",
    "            low=self.observation_space.low.min(),\n",
    "            high=self.observation_space.high.max(),\n",
    "            shape=(obs_shape[2], obs_shape[0], obs_shape[1]),\n",
    "            dtype=env.observation_space.dtype,\n",
    "        )\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.transpose(observation, (2, 0, 1))\n",
    "\n",
    "class SmallBox84(ObservationWrapper): # Resize to 84x84\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=1.0,\n",
    "            shape=(1, 84, 84),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "    def observation(self, observation):\n",
    "        return cv2.resize(observation, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "\n",
    "class GreyScale(ObservationWrapper): # Convert to greyscale\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.observation_space = Box(\n",
    "            low=0,\n",
    "            high=1.0,\n",
    "            shape=(1, env.observation_space.shape[1], env.observation_space.shape[2]),\n",
    "            dtype=np.uint8,\n",
    "        )\n",
    "    def transform(self, observation):\n",
    "        return np.dot(observation[..., :3], [0.299, 0.587, 0.114]).reshape(1, observation.shape[0], observation.shape[1])\n",
    "\n",
    "    def observation(self, observation):\n",
    "        observation = self.transform(observation)\n",
    "        return observation.astype(np.float32) / 255.0\n",
    "    \n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v2', apply_api_compatibility=True, render_mode='robot')\n",
    "env = JoypadSpace(env, actions.SIMPLE_MOVEMENT)\n",
    "env = ReShape(env)\n",
    "env = GreyScale(env)\n",
    "env = SmallBox84(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b99c1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "gamma = 0.95 # discount reward\n",
    "update = 0.004 # learning rate\n",
    "epsilon_start = 0.9 # initial exploration rate\n",
    "epsilon_end = 0.005 # final exploration rate\n",
    "e_decay = 2500 # decay rate\n",
    "action_n = env.action_space.n\n",
    "obs, info = env.reset()\n",
    "\n",
    "network = DQN_full(obs.shape, action_n).to(system)\n",
    "\n",
    "optimiser = opt.AdamW(network.parameters(), lr=update)\n",
    "memory = ReplayMemory(1000) \n",
    "\n",
    "steps_passed = 0\n",
    "\n",
    "def select_action(state):\n",
    "    global steps_passed\n",
    "    sample = random.random()\n",
    "    steps_passed += 1\n",
    "    epsilon = epsilon_end + (epsilon_start - epsilon_end) * math.exp(-1. * steps_passed / e_decay)\n",
    "    if sample > epsilon:\n",
    "        with torch.no_grad():\n",
    "            return network(state).max(1)[1].view(1, 1)\n",
    "    else:\n",
    "        return torch.tensor([[env.action_space.sample()]], device=system, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ba0889",
   "metadata": {},
   "source": [
    "## Model Optimiser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "876ad882",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 50 #UPDATE\n",
    "\n",
    "def optimize_model():\n",
    "    if len(memory) < batch_size: # take 250 of 10000 to update model params\n",
    "        return\n",
    "    transitions = memory.sample(batch_size)\n",
    "    batch = Transition(*zip(*transitions))\n",
    "\n",
    "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n",
    "                                          batch.next_state)), device=system, dtype=torch.bool)\n",
    "    non_final_next_states = torch.cat([s for s in batch.next_state\n",
    "                                                if s is not None])\n",
    "    state_batch = torch.cat(batch.state)\n",
    "    action_batch = torch.cat(batch.action)\n",
    "    reward_batch = torch.cat(batch.reward)\n",
    "\n",
    "    state_action_values = network(state_batch).gather(1, action_batch)\n",
    "\n",
    "    next_state_values = torch.zeros(batch_size, device=system)\n",
    "    with torch.no_grad():\n",
    "        next_state_values[non_final_mask] = network(non_final_next_states).max(1).values\n",
    "    \n",
    "    # Compute expected Q values\n",
    "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
    "\n",
    "    criterion = nn.SmoothL1Loss()\n",
    "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "\n",
    "    optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # In-place gradient clipping\n",
    "    torch.nn.utils.clip_grad_value_(network.parameters(), 15)\n",
    "    optimiser.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "617f0dd3",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23763a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  0\n",
      "tensor([164.])\n",
      "Starting Episode  1\n",
      "tensor([175.])\n",
      "Starting Episode  2\n",
      "tensor([155.])\n",
      "Starting Episode  3\n",
      "tensor([148.])\n",
      "Starting Episode  4\n",
      "tensor([136.])\n",
      "Starting Episode  5\n",
      "tensor([141.])\n",
      "Starting Episode  6\n",
      "tensor([170.])\n",
      "Starting Episode  7\n",
      "tensor([171.])\n",
      "Starting Episode  8\n",
      "tensor([137.])\n",
      "Starting Episode  9\n",
      "tensor([215.])\n",
      "Starting Episode  10\n",
      "tensor([196.])\n",
      "Starting Episode  11\n",
      "tensor([197.])\n",
      "Starting Episode  12\n",
      "tensor([172.])\n",
      "Starting Episode  13\n",
      "tensor([168.])\n",
      "Starting Episode  14\n",
      "tensor([184.])\n",
      "Done Training\n"
     ]
    }
   ],
   "source": [
    "if system == torch.device(\"cuda\"):\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 15\n",
    "\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "steps_passed = 0\n",
    "\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state.copy(), dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    done = None\n",
    "    for i in range(1, 100):  # count():\n",
    "        action = select_action(state)\n",
    "        obs, reward, terminated, truncated, info = env.step(action.item())\n",
    "        reward = torch.tensor([reward], device=system)\n",
    "        done = terminated or truncated\n",
    "        total_reward += reward\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "\n",
    "        memory.push(state, action, next_state, reward)\n",
    "        state = next_state\n",
    "\n",
    "        optimize_model()\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "        \n",
    "    episode_rewards.append(total_reward)\n",
    "    print(total_reward)\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "torch.save(network.state_dict(), 'mario_DQN.pth')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346f2571",
   "metadata": {},
   "source": [
    "# /// A2C Algorithm ///\n",
    "\n",
    "To save time, we will use some of the same classes and functions from DQN. Make sure you run all of the prior code (except the training loop for DQN)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "31612832",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Episode  0\n",
      "632.0\n",
      "Starting Episode  1\n",
      "680.0\n",
      "Starting Episode  2\n",
      "680.0\n",
      "Starting Episode  3\n",
      "680.0\n",
      "Starting Episode  4\n",
      "-90.0\n",
      "Starting Episode  5\n",
      "577.0\n",
      "Starting Episode  6\n",
      "297.0\n",
      "Starting Episode  7\n",
      "504.0\n",
      "Starting Episode  8\n",
      "632.0\n",
      "Starting Episode  9\n",
      "-43.0\n",
      "Starting Episode  10\n",
      "632.0\n",
      "Starting Episode  11\n",
      "632.0\n",
      "Starting Episode  12\n",
      "632.0\n",
      "Starting Episode  13\n",
      "1262.0\n",
      "Starting Episode  14\n",
      "1123.0\n",
      "Starting Episode  15\n",
      "1302.0\n",
      "Starting Episode  16\n",
      "1363.0\n",
      "Starting Episode  17\n",
      "1284.0\n",
      "Starting Episode  18\n",
      "1451.0\n",
      "Starting Episode  19\n",
      "680.0\n",
      "Starting Episode  20\n",
      "680.0\n",
      "Starting Episode  21\n",
      "825.0\n",
      "Starting Episode  22\n",
      "680.0\n",
      "Starting Episode  23\n",
      "680.0\n",
      "Starting Episode  24\n",
      "680.0\n",
      "Starting Episode  25\n",
      "680.0\n",
      "Starting Episode  26\n",
      "680.0\n",
      "Starting Episode  27\n",
      "680.0\n",
      "Starting Episode  28\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_976\\4153601337.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[0mnext_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munsqueeze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "steps_passed = 0\n",
    "episode_rewards = []\n",
    "episode_steps = []\n",
    "gamma = 0.99\n",
    "a2c_update = 0.05\n",
    "\n",
    "if system == torch.device(\"cuda\"):\n",
    "    episodes = 600\n",
    "else:\n",
    "    episodes = 40\n",
    "\n",
    "\n",
    "class CriticNetwork(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, actions_n):\n",
    "        super(CriticNetwork, self).__init__()\n",
    "        self.convolution = nn.Sequential( # combined convolutional layers\n",
    "        nn.Conv2d(shape[0],32, kernel_size=8, stride=4),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "        nn.ReLU(),\n",
    "        nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "        nn.ReLU()\n",
    "        )\n",
    "\n",
    "        output_size = self._get_conv_out(shape)\n",
    "        self.actor = nn.Sequential(  \n",
    "            nn.Linear(output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, actions_n),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.critic = nn.Sequential(\n",
    "            nn.Linear(output_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "\n",
    "    def _get_conv_out(self, shape):\n",
    "        with torch.no_grad():\n",
    "            blank = torch.zeros(1, *shape)\n",
    "            out = self.convolution(blank)\n",
    "            return int(np.prod(out.size())) \n",
    "\n",
    "    def get_values(self, x): # for debugging\n",
    "        x = self.convolution(x)\n",
    "        actor = self.actor(x).view(x.size(0), -1)\n",
    "        critic = self.critic(x).view(x.size(0), -1)\n",
    "        return actor, critic\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.convolution(x).view(x.size(0), -1)\n",
    "        probs = nn.Softmax(dim=1)(self.actor(x))\n",
    "        state_values = self.critic(x)\n",
    "        return probs, state_values\n",
    "\n",
    "\n",
    "a2c_model = CriticNetwork(obs.shape, action_n).to(system)\n",
    "a2c_optimiser = opt.Adam(a2c_model.parameters(), lr=a2c_update)\n",
    "\n",
    "\n",
    "for episode_n in range(episodes):\n",
    "    print(\"Starting Episode \", episode_n)\n",
    "    state, info = env.reset()\n",
    "    state = torch.tensor(state.copy(), dtype=torch.float32, device=system).unsqueeze(0)\n",
    "    total_reward = 0\n",
    "    probability_logs = []\n",
    "    rewards = []\n",
    "    values = []\n",
    "    for i in range(1, 1000):  # count():\n",
    "        action_probability, value = a2c_model(state)\n",
    "        action = torch.multinomial(action_probability, 1).item()\n",
    "        prob_log = torch.log(action_probability.squeeze(0)[action])\n",
    "        obs, reward, terminated, truncated, info = env.step(action)\n",
    "        next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "        probability_logs.append(prob_log)\n",
    "        total_reward += reward\n",
    "        rewards.append(reward)\n",
    "        values.append(value)\n",
    "        done = terminated or truncated\n",
    "        if terminated:\n",
    "            next_state = None\n",
    "        else:\n",
    "            next_state = torch.tensor(obs, dtype=torch.float32, device=system).unsqueeze(0)\n",
    "        state = next_state\n",
    "\n",
    "        if done:\n",
    "            break\n",
    "    episode_steps.append(i+1)\n",
    "    episode_rewards.append(total_reward)\n",
    "    print(total_reward)\n",
    "\n",
    "\n",
    "    returns = []\n",
    "    discounted_return = 0\n",
    "    for reward in reversed(rewards):\n",
    "        discounted_return = reward + gamma * discounted_return\n",
    "        returns.insert(0, discounted_return)\n",
    "    returns = torch.tensor(returns, dtype=torch.float32, device=system)\n",
    "\n",
    "    probability_logs = torch.stack(probability_logs)\n",
    "    values = torch.cat(values).squeeze()\n",
    "    advantage = returns - values\n",
    "\n",
    "    actor_loss = -(probability_logs * advantage.detach()).mean()\n",
    "    critic_loss = advantage.pow(2).mean()\n",
    "\n",
    "    loss = actor_loss + critic_loss\n",
    "    a2c_optimiser.zero_grad()\n",
    "    loss.backward()\n",
    "    torch.nn.utils.clip_grad_norm_(a2c_model.parameters(), 0.5)\n",
    "    a2c_optimiser.step()\n",
    "\n",
    "print('Done Training')\n",
    "\n",
    "torch.save(a2c_model.state_dict(), 'mario_A2C.pth')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
