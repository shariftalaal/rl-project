{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eef65dca",
   "metadata": {},
   "source": [
    "# Super Mario Bros RL Notebook\n",
    "## Rohan H, Talaal S, Thang N, Yuet W\n",
    "### University of Bath\n",
    "\n",
    "![SegmentLocal](mario!.gif \"segment\")\n",
    "(we are going to have to delete him sadly...)\n",
    "\n",
    "\n",
    "### How to run:\n",
    "The official website for the game environment can be found here: https://pypi.org/project/gym-super-mario-bros/\n",
    "\n",
    "In a nutshell, you will need:\n",
    "- Python 3.5/3.6/3.7/3.8 (I have tested on 3.7)\n",
    "- gymnasium (gym is deprecated)\n",
    "- ipykernel for running the notebook\n",
    "- gym-super-mario-bros \n",
    "- other essential packages/libraries like NumPy\n",
    "- an average computer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2bcfa068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
      "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
      "Users of this version of Gym should be able to simply replace 'import gym' with 'import gymnasium as gym' in the vast majority of cases.\n",
      "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:556: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  f\"The environment {id} is out of date. You should consider \"\n",
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "0.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "-1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "0.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n",
      "2.0\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from nes_py.wrappers import JoypadSpace\n",
    "import time\n",
    "import gym_super_mario_bros\n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT\n",
    "env_example = gym_super_mario_bros.make('SuperMarioBros-v0', apply_api_compatibility=True, render_mode='human')\n",
    "env_example = JoypadSpace(env_example, SIMPLE_MOVEMENT)\n",
    "\n",
    "done = True\n",
    "for step in range(50):\n",
    "    if done:\n",
    "        obs, info = env_example.reset()\n",
    "    obs, reward, terminated, truncated, info = env_example.step(env_example.action_space.sample())\n",
    "    done = terminated or truncated\n",
    "    print(reward)\n",
    "    time.sleep(0.2)\n",
    "\n",
    "env_example.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e96786cb",
   "metadata": {},
   "source": [
    "## Independent Work\n",
    "\n",
    "Each of the cells below will belong to each member of the group. If you have independent work to be getting on with, you should do it in one of the cells below. Branches are yet to be organised, either by member or by task. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f26537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Talaal's Cell\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8df5bad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\robot\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\gym\\envs\\registration.py:628: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  f\"The environment creator metadata doesn't include `render_modes`, contains: {list(env_creator.metadata.keys())}\"\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'Discrete' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2388\\1330010275.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    104\u001b[0m         \u001b[1;31m# Update cumulative reward for episode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 105\u001b[1;33m         \u001b[0mstate_action_pairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msum_rewards\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmcagent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_episode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    106\u001b[0m         \u001b[0mmcagent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlearn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_action_pairs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnext_states\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2388\\1330010275.py\u001b[0m in \u001b[0;36mgenerate_episode\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[1;31m# Generate a full episode of experience.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mterminal\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mterminal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_2388\\1330010275.py\u001b[0m in \u001b[0;36mpolicy\u001b[1;34m(self, state, greedy)\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mpolicy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgreedy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[1;31m# Get the list of actions\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mavailable_actions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[1;31m# Shuffle the list of actions to act as a random tie-breaker, as\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Discrete' object is not callable"
     ]
    }
   ],
   "source": [
    "# Rohan's Cell\n",
    "\n",
    "# YOUR CODE HERE\n",
    "import numpy as np\n",
    "from statistics import mean\n",
    "import random\n",
    "\n",
    "class MCAgent(object) :\n",
    "    \n",
    "    def __init__(self, env, epsilon = 0.15, gamma = 0.9) :\n",
    "\n",
    "        self.env = env\n",
    "        self.returns = {}\n",
    "        self.epsilon = epsilon\n",
    "        self.gamma = gamma\n",
    "    \n",
    "    def policy(self, state, greedy = False) :\n",
    "        # Get the list of actions \n",
    "        available_actions = self.env.action_space().copy()\n",
    "        \n",
    "        # Shuffle the list of actions to act as a random tie-breaker, as\n",
    "        # list.index always returns the first matching element of a list.\n",
    "        random.shuffle(available_actions)\n",
    "        \n",
    "        # Take random action.\n",
    "        if (greedy == False and random.random() < self.epsilon) :\n",
    "            return random.choice(available_actions)\n",
    "        # Take greedy action.\n",
    "        else :\n",
    "            q_table = {(state, a) : mean(self.returns.get((state, a), [0])) for a in available_actions}\n",
    "            q_values = [q_table.get((state, a), 0) for a in available_actions]\n",
    "            return available_actions[q_values.index(max(q_values))]\n",
    "        \n",
    "    def learn(self, state_action_pairs, rewards, next_states) :\n",
    "        \n",
    "        episode_return = 0\n",
    "        \n",
    "        # Loop through our episode experience (backwards, as per the pseudocode).\n",
    "        while(len(state_action_pairs) > 0) :\n",
    "            \n",
    "            # We \"pop\" the last time-step from our experience lists\n",
    "            # each iteration until they are empty.\n",
    "            state, action = state_action_pairs.pop()\n",
    "            reward = rewards.pop()\n",
    "            next_state = next_states.pop()\n",
    "            \n",
    "            # Update the return earned after this time-step.\n",
    "            episode_return = reward + self.gamma * episode_return\n",
    "            \n",
    "            # If this is our first-visit to this state-action pair in this\n",
    "            # episode, update its list of returns.\n",
    "            if (not (state, action) in state_action_pairs) :\n",
    "                returns_list = self.returns.get((state, action), [])\n",
    "                returns_list.append(episode_return)\n",
    "                self.returns[(state, action)] = returns_list.copy()\n",
    "            \n",
    "    def generate_episode(self) :\n",
    "        \n",
    "        # Initialise variables for storing our agent's experience.\n",
    "        state_action_pairs = []\n",
    "        rewards = []\n",
    "        next_states = []\n",
    "        summed_rewards = 0\n",
    "        \n",
    "        # Initialise environment.\n",
    "        state = self.env.reset()\n",
    "        terminal = False\n",
    "        \n",
    "        # Generate a full episode of experience.\n",
    "        while (not terminal) :\n",
    "            action = self.policy(state)\n",
    "            next_state, reward, terminal = self.env.step(action)\n",
    "            \n",
    "            state_action_pairs.append((state, action))\n",
    "            rewards.append(reward)\n",
    "            next_states.append(next_state)\n",
    "            \n",
    "            state = next_state\n",
    "            \n",
    "            summed_rewards += reward\n",
    "            \n",
    "            if (terminal) :\n",
    "                break\n",
    "                \n",
    "        return state_action_pairs, rewards, next_states, summed_rewards\n",
    "    \n",
    "\n",
    "# Run 20 agents for 150 episodes, recording per-episode reward.\n",
    "num_agents = 20\n",
    "num_episodes = 150\n",
    "mc_rewards = []\n",
    "\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v3', apply_api_compatibility=True, render_mode='robot')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "\n",
    "for agent in range(0, num_agents) :\n",
    "    #print(\"Agent {}\".format(agent + 1))\n",
    "    episode_rewards = []\n",
    "\n",
    "    mcagent = MCAgent(env)\n",
    "    \n",
    "    for episode in range(0, num_episodes) :\n",
    "\n",
    "        # Update cumulative reward for episode.\n",
    "        state_action_pairs, rewards, next_states, sum_rewards = mcagent.generate_episode()\n",
    "        mcagent.learn(state_action_pairs, rewards, next_states)\n",
    "\n",
    "        episode_rewards.append(sum_rewards)\n",
    "    mc_rewards.append(episode_rewards)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a2b17b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thang's Cell\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ef0c58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Yuet's Cell\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
